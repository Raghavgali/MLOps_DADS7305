# Spam Tutorial Notebook Overview

This notebook walks through building a weak supervision pipeline on a subset of the IMDb movie review dataset. It mirrors the flow of Snorkel's spam tutorial but adapts it for local experimentation on hardware with limited free storage.

## Key steps
- Downsample the full dataset to 5,000 stratified examples to keep runtime and disk usage low.
- Clean review text and create train/test splits used across the tutorial.
- Author labeling functions (LFs) ranging from simple keyword rules to TextBlob and spaCy-based heuristics.
- Apply LFs sequentially in small batches to avoid large temporary files and excessive memory pressure.
- Combine LF outputs with Snorkel's `LabelModel` and train discriminative models on the resulting probabilistic labels.

## Labeling functions
- **Heuristic keywords**: Early LFs (`mentions_love`, `mentions_loved_it`) check for high-signal phrases such as “loved it.” They operate directly on the raw text column and return `POSITIVE`, `NEGATIVE`, or `ABSTAIN`.
- **Regex patterns**: Rules like `regex_not_good` capture reusable negative phrasing (e.g., “not good”). Regex-based LFs expand coverage beyond exact keywords while remaining fast.
- **Third-party sentiment models**: The TextBlob preprocessors compute polarity and subjectivity once per example. Two LFs threshold those scores (`textblob_positive`, `textblob_negative`) to translate continuous polarity into discrete weak labels.
- **Structured NLP cues**: The spaCy preprocessor enriches each row with a `doc` object so LFs can reference entity spans and token counts. `mentions_character` and `mentions_character_nlp` leverage that structure to flag longer reviews praising specific actors.
- **Sanity and balance checks**: Additional rules such as `short_review` target simple distributional patterns (very short reviews often signal frustration), ensuring the LF set expresses a mix of positive and negative evidence.

Every LF is decorated with Snorkel’s `@labeling_function` helper so they share a uniform interface. Preprocessors disable memoization to avoid storing large caches on disk. The helper `apply_lfs_sequentially` feeds only the `text` column to these LFs in 512-row batches, yielding NumPy label matrices while keeping I/O and memory low.

## Practical adjustments
- Set `JOBLIB_MULTIPROCESSING=0` and process data batches of 512 rows to prevent multi-process pickling from exhausting disk space.
- Disable memoization for heavyweight preprocessors so cached artifacts do not accumulate on disk during repeated runs.
- Limit LF inputs to the minimal `text` column to shrink intermediate objects.

Together these changes ensure the notebook stays responsive on machines with limited free storage while preserving the learning objectives of the original walkthrough.

## Environment notes
- Python stack: pandas, scikit-learn, Snorkel, TextBlob, spaCy (`en_core_web_sm`). The notebook installs or imports these within the relevant cells.
- GPU acceleration is not required; all steps run comfortably on CPU once the dataset is trimmed.
- Expect around 4–5 minutes to execute all cells on a typical laptop after the spaCy model download completes.

## What you get after running the notebook
- Weak labels (`L_train`, `L_test`) generated by the sequential LF applier.
- A trained Snorkel `LabelModel` fitted on the weak labels.
- Baseline discriminative models (logistic regression / neural nets) trained and evaluated on probabilistic labels to demonstrate downstream gains.

## How to use it
1. Create and activate a Python environment with the dependencies above.
2. Launch JupyterLab or VS Code and open `01_spam_tutorial.ipynb`.
3. Run cells top to bottom; rerun the data-loading cell if you want a fresh 5,000-sample draw.
4. Inspect the LF analysis and model evaluation cells to understand how each adjustment affects coverage, conflicts, and accuracy.
